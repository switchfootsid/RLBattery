{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API for Battery Control Agent\n",
    "\n",
    "### Discretization code only inside function approximation or Q-update module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from __future__ import division\n",
    "#%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-64-7e5d915023b3>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-7e5d915023b3>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    self.qvalue = defaultdict(lambda: -0.00 * np.random.randn(len(self.actions)) 0) #negative qvalues for 12 actions\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class QLearning:\n",
    "    def __init__ (self, E_cap, P_cap, efficiency, gamma, learning_rate):\n",
    "        \n",
    "        #-----------Battery parameters-----------------\n",
    "        self.E_max = E_cap\n",
    "        self.P_cap = P_cap\n",
    "        self.E_min = (1-0.80)*self.E_max #depth of discharge\n",
    "        self.E_init = (0.30)*self.E_max #30% of charge every morning \n",
    "        self.eta = efficiency\n",
    "        \n",
    "        #-----------Q-learning Hyperparameters---------\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.qvalue = defaultdict(lambda: -0.00 * np.random.randn(len(self.actions)) 0) #negative qvalues for 12 actions\n",
    "        self.epsilon = 0.1\n",
    "        #-----------Steps-------------------------------\n",
    "        self.action_step = 0.10*P_cap\n",
    "        self.energy_step = 0.30\n",
    "        self.netload_step = 0.30\n",
    "        self.actions = np.arange(-self.P_cap, self.P_cap, self.action_step)\n",
    "                \n",
    "        #------------Discrete Bins---------------------\n",
    "        self.energy_bins = np.arange(1.2, self.E_max + 0.01, self.energy_step)\n",
    "        self.netload_bins = np.arange(-2.6, 5.5, self.netload_step)\n",
    "        self.price = self.price = np.array([.040,.040,.040,.040,.040,.040,.080,.080,.080,.080,.040,.040,.080,.080,.080,.040,.040,.120,.120,.040,.040,.040,.040,.040]).tolist()\n",
    "\n",
    "    def get_action(self, currentState):\n",
    "        '''\n",
    "        Returns an action according to an epsilon-greedy policy\n",
    "        Output: action, action_index\n",
    "        '''\n",
    "        legal_actions = self.get_legal_actions(currentState)\n",
    "        currentState_index = self.discretize_state(currentState)\n",
    "        \n",
    "        if np.random.random() <= self.epsilon:\n",
    "            action_index = random.choice(legal_actions)\n",
    "            action = self.actions[action_index]\n",
    "        else:\n",
    "            qlist = self.qvalue[currentState_index]\n",
    "            qlist = [qlist[k] for k in legal_actions]\n",
    "            index = np.nanargmax(qlist) \n",
    "            action_index = legal_actions[index]\n",
    "            action = self.actions[action_index]\n",
    "            \n",
    "        return action, action_index\n",
    "    \n",
    "    def get_legal_actions(self, currentState):\n",
    "        '''\n",
    "        Calculate and return allowable action set\n",
    "        Output: List of indices of allowable actions\n",
    "        '''\n",
    "        energy_level = currentState[1]\n",
    "        #energy_level = self.energy_bins((np.digitize(energy_level, self.energy_bins)-1))\n",
    "        lower_bound = max(self.E_min - energy_level, -self.P_cap)\n",
    "        upper_bound = min(self.E_max - energy_level, self.P_cap)\n",
    "        max_bin = int(np.digitize(upper_bound, self.actions, right=True)) ###\n",
    "        min_bin = int(np.digitize(lower_bound, self.actions, right=True)) ###\n",
    "        \n",
    "        legal_actions = []\n",
    "        \n",
    "        for k in range(min_bin, max_bin+1):\n",
    "            legal_actions.append(k)\n",
    "        return legal_actions\n",
    "     \n",
    "    def discretize_state(self, state):\n",
    "        '''\n",
    "        Discretize state variables individually and return an index tuple for hashing Q[(s,a)] function.\n",
    "        Return: bin values.\n",
    "        '''\n",
    "        load_index = int(np.digitize(state[0], self.netload_bins, right=True)) ###\n",
    "        energy_index = int(np.digitize(state[1], self.energy_bins, right=True)) ###\n",
    "        state_index = (load_index, energy_index)\n",
    "        \n",
    "        return state_index\n",
    "    \n",
    "    def next_step(self, currentState, action, time_step):\n",
    "        '''\n",
    "        Perform constraint checking (energy, grid) and assign penalty/rewards.\n",
    "        Output: reward/penalty, next state, constraint satisfaction (boolean)\n",
    "        '''\n",
    "        current_netload = currentState[0]\n",
    "        current_energy = currentState[1]\n",
    "        \n",
    "        if action >= 0:\n",
    "            P_charge, P_discharge = action, 0.0\n",
    "        else:\n",
    "            P_charge, P_discharge = 0.0, action\n",
    "        \n",
    "        E_next = current_energy + self.eta * P_charge + P_discharge\n",
    "        P_grid = current_netload + P_charge + P_discharge\n",
    "        \n",
    "        condition1 = bool((E_next >= self.E_min) and (E_next <= self.E_max))\n",
    "\n",
    "        if condition1 != True:\n",
    "            print action, E_next\n",
    "            print 'hello'\n",
    "        condition2 = bool(P_grid >= 0)\n",
    "        #condition = (P_grid >= 0)\n",
    "        \n",
    "        if P_grid >= 0:\n",
    "            reward = -P_grid*self.price[time_step] # -P_grid*self.price[time_step]\n",
    "            #print current_energy, action, E_next\n",
    "            nextState = (0.0, E_next)\n",
    "        elif P_grid < 0:\n",
    "            reward = P_grid*10\n",
    "            nextState = None\n",
    "        \n",
    "        condition = condition1 and condition2\n",
    "        \n",
    "        return reward, nextState, P_grid, condition\n",
    "    \n",
    "    def update_Qvalue(self, currentState, action_index, reward, nextState, condition):\n",
    "        '''\n",
    "        Performs a Q-learning update.\n",
    "            if condition = False, penalize\n",
    "            if condition = True, update Q-value. \n",
    "        \n",
    "        :type currentState, nextState: list of ints\n",
    "        :param currentState, nextState: \n",
    "\n",
    "        :type reward: float\n",
    "        :param reward: immediate reward associated with transitioning to nextState\n",
    "        \n",
    "        '''\n",
    "        if condition == False:\n",
    "            currentState_index = self.discretize_state(currentState)\n",
    "            td_error = reward - self.qvalue[currentState_index][action_index]\n",
    "            self.qvalue[currentState_index][action_index] = 0.50*td_error\n",
    "            return None\n",
    "        \n",
    "        currentState_index = self.discretize_state(currentState)\n",
    "        nextState_index = self.discretize_state(nextState)\n",
    "        nextAction_index = np.nanargmax(self.qvalue[nextState_index])\n",
    "        \n",
    "        td_error = self.gamma*self.qvalue[nextState_index][nextAction_index] - self.qvalue[currentState_index][action_index] \n",
    "        self.qvalue[currentState_index][action_index] += self.alpha *(reward + td_error) \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def update_Qvalue_approx(self, currentState, action, reward, nextState, condition):\n",
    "        '''\n",
    "        Use function approximation to get Q-value of currentState and perform Q-update.\n",
    "        Q table = defaultdict.\n",
    "        '''\n",
    "        return None\n",
    "    \n",
    "    def greedy_action(self, currentState):\n",
    "        currentState_index = self.discretize_state(currentState)\n",
    "        legal_actions = self.get_legal_actions(currentState)\n",
    "        qlist = self.qvalue[currentState_index]\n",
    "        qlist = [qlist[k] for k in legal_actions]\n",
    "        index = np.nanargmax(qlist) \n",
    "        action_index = legal_actions[index]\n",
    "        action = self.actions[action_index]\n",
    "        return action, action_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000133333333295\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    def episode(day_states, ql, grid_profile):\n",
    "        currentState = day_states[0]\n",
    "        currentState[1] = ql.E_init\n",
    "        grid_list = grid_profile\n",
    "        reward_list = list() #episodic rewards\n",
    "        cumulative_reward = 0.0\n",
    "        \n",
    "        for t in range(23):\n",
    "            time_step = t+1\n",
    "            action, action_index = ql.get_action(currentState)\n",
    "            reward, nextState, P_grid, condition = ql.next_step(currentState, action, t)\n",
    "            \n",
    "            if nextState != None:\n",
    "                E_next = nextState[1]\n",
    "                nextState = day_states[time_step]\n",
    "                nextState[1] = E_next            \n",
    "            \n",
    "            ql.update_Qvalue(currentState, action_index, reward, nextState, condition)\n",
    "            \n",
    "            if condition == False:\n",
    "                grid_list.append(P_grid)\n",
    "                break\n",
    "            \n",
    "            currentState = nextState\n",
    "            grid_list.append(P_grid)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "        reward_list.append(cumulative_reward)\n",
    "        return grid_list, cumulative_reward\n",
    "    \n",
    "    ql = QLearning(6.0, 3.0, 0.90, 0.90, 0.5)\n",
    "    YEARS = 500\n",
    "    total_days = 15*YEARS\n",
    "    df_solar = pd.read_csv('/Users/siddharth481/Documents/IITBombay/Battery/SolarData/solar_clean.csv')\n",
    "    df_load = pd.read_csv('/Users/siddharth481/Documents/IITBombay/Battery/SolarData/load_data_peak6.csv')\n",
    "    diff = (df_load.ix[0:14] - df_solar.ix[0:14])\n",
    "    net_load = pd.concat(([diff.multiply(1)])*YEARS, ignore_index=True).values.tolist()\n",
    "    ql.epsilon = 0.7\n",
    "    count = 1\n",
    "    grid_profile = list()\n",
    "    reward_list = list()\n",
    "    \n",
    "    for episode_day in xrange(total_days):\n",
    "        E_st = np.zeros(24)\n",
    "        weekly_bill = 0.0\n",
    "        ##S = [net_load[episode_day], E_st, ql.price]\n",
    "        S = [net_load[episode_day], E_st]\n",
    "        day_states = map(list, zip(*S))\n",
    "        grid_profile, reward = episode(day_states, ql, grid_profile)\n",
    "        \n",
    "        if count == 7:\n",
    "            reward_list.append(-1*reward)\n",
    "            weekly_bill = 0.0\n",
    "            count = 1\n",
    "        else:\n",
    "            count +=1\n",
    "        \n",
    "        if ql.epsilon >= 0:\n",
    "            ql.epsilon -= 1/total_days\n",
    "    plt.plot(grid_profile)#make the plot\n",
    "    print ql.epsilon\n",
    "    plt.show()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_days = 15\n",
    "ql = ql\n",
    "grid_prof = list()\n",
    "reward_list = list()\n",
    "action_list = list()\n",
    "\n",
    "def play_episode(day_states, ql, grid_profile):\n",
    "\n",
    "    currentState = day_states[0]\n",
    "    currentState[1] = ql.E_init\n",
    "    grid_list = grid_profile\n",
    "    reward_list = list() \n",
    "    cumulative_reward = 0.0\n",
    "    \n",
    "    for t in range(24):\n",
    "        time_step = t+1\n",
    "        action, action_index = ql.greedy_action(currentState)\n",
    "        index = ql.discretize_state(currentState)\n",
    "        #print ql.qvalue[index], action_index\n",
    "        action_list.append(action)\n",
    "        reward, nextState, P_grid, condition = ql.next_step(currentState, action, t)\n",
    "        #print currentState[1], action, nextState[1]\n",
    "        #print '\\n'\n",
    "\n",
    "        if (nextState != None and (time_step < 23)):\n",
    "            E_next = nextState[1]\n",
    "            nextState = day_states[time_step]\n",
    "            nextState[1] = E_next \n",
    "            currentState = nextState\n",
    "            \n",
    "        if condition == False:\n",
    "            grid_list.append(P_grid)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "        grid_list.append(P_grid)\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    reward_list.append(cumulative_reward)\n",
    "    return grid_list, cumulative_reward\n",
    "\n",
    "for episode_day in xrange(total_days):\n",
    "    E_st = np.zeros(24)\n",
    "    S = [net_load[episode_day], E_st]\n",
    "    day_states = map(list, zip(*S))\n",
    "    grid_prof, reward = play_episode(day_states, ql, grid_prof)\n",
    "        \n",
    "    '''\n",
    "    if count == 7:\n",
    "        rewards.append(weekly_bill)\n",
    "        weekly_bill = 0.0\n",
    "        count = 1\n",
    "    else:\n",
    "        count +=1\n",
    "    '''\n",
    "    reward_list.append(reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(grid_prof)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 6.0, 6.0, 6.0, 0.0, 0.0, 6.0, 6.0, 6.0, 0.0, 0.0, 12.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "TOU = [.040,.040,.040,.040,.040,.040,.080,.080,.080,.080,.040,.040,.080,.080,.080,.040,.040,.120,.120,.040,.040,.040,.040,.040]\n",
    "scaled_TOU = list()\n",
    "for price in TOU:\n",
    "    scaled_TOU.append(price*150 - 6)\n",
    "print scaled_TOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower_margin = 3\n",
    "upper_margin = 7\n",
    "week = 0 #35*7\n",
    "plt.plot([x*-1 for x in action_list[24*(week + lower_margin):24*(week + upper_margin)]], 'g')\n",
    "plt.plot(scaled_TOU*(upper_margin - lower_margin), 'c')\n",
    "plt.plot(df_solar.iloc[(week + lower_margin):(week + upper_margin)].values.flatten(), 'y')\n",
    "plt.plot(df_load.iloc[(week + lower_margin):(week + upper_margin)].values.flatten(), 'b')\n",
    "plt.plot(grid_prof[24*(week + lower_margin):24*(week + upper_margin)], 'r')\n",
    "# Load = Solar + Grid - (-Discharge + Charge)\n",
    "# 2.4 = 5.5 + Grid - 3.0\n",
    "#plt.plot(map(sum, zip(grid_prof[100*24:24*105],[x*-1 for x in action_list[24*100:24*105]], df_solar.iloc[100:105].values.flatten().tolist())), 'r')\n",
    "plt.xlabel('Time in Hours')\n",
    "plt.ylabel('Power kW')\n",
    "plt.title('Weekday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### bins = np.arange(-3.,3.0, 0.5)\n",
    "print bins\n",
    "bins[np.digitize(2.3, bins, right=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(-2.6,5.201,0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
